ok: class CLASS ::.Dnn : , name: Dnn, base: 

===== Common header : /home/jeon/다운로드/opencv-3.4.0/modules/dnn/misc/java/src/cpp/dnn_converters.hpp =====


===== Header: /home/jeon/다운로드/opencv-3.4.0/modules/dnn/include/opencv2/dnn.hpp =====
Namespaces: set([])
Ignore header: /home/jeon/다운로드/opencv-3.4.0/modules/dnn/include/opencv2/dnn.hpp


===== Header: /home/jeon/다운로드/opencv-3.4.0/modules/dnn/include/opencv2/dnn/layer.details.hpp =====
Namespaces: set([u'cv.dnn', u'cv', u'cv.dnn.details'])
Ignore header: /home/jeon/다운로드/opencv-3.4.0/modules/dnn/include/opencv2/dnn/layer.details.hpp


===== Header: /home/jeon/다운로드/opencv-3.4.0/modules/dnn/include/opencv2/dnn/dnn.inl.hpp =====
Namespaces: set([u'cv.dnn', u'cv', u'cv.dnn.details'])
Ignore header: /home/jeon/다운로드/opencv-3.4.0/modules/dnn/include/opencv2/dnn/dnn.inl.hpp


===== Header: /home/jeon/다운로드/opencv-3.4.0/modules/dnn/include/opencv2/dnn/shape_utils.hpp =====
Namespaces: set([u'cv.dnn.<unnamed>', u'cv.dnn', u'cv', u'cv.dnn.details'])
Ignore header: /home/jeon/다운로드/opencv-3.4.0/modules/dnn/include/opencv2/dnn/shape_utils.hpp


===== Header: /home/jeon/다운로드/opencv-3.4.0/modules/dnn/include/opencv2/dnn/dnn.hpp =====
Namespaces: set([u'cv.dnn.<unnamed>', u'cv.dnn.experimental_dnn_v3', u'cv.dnn', u'cv', u'cv.dnn.details'])

--- Incoming ---
[u'const cv.dnn.DNN_BACKEND_DEFAULT', '0', [], [], None, '']
ok: CONST DNN_BACKEND_DEFAULT=0

--- Incoming ---
[u'const cv.dnn.DNN_BACKEND_HALIDE', '1', [], [], None, '']
ok: CONST DNN_BACKEND_HALIDE=1

--- Incoming ---
[u'const cv.dnn.DNN_TARGET_CPU', '0', [], [], None, '']
ok: CONST DNN_TARGET_CPU=0

--- Incoming ---
[u'const cv.dnn.DNN_TARGET_OPENCL', '1', [], [], None, '']
ok: CONST DNN_TARGET_OPENCL=1

--- Incoming ---
[   u'class cv.dnn.Layer',
    ': cv::Algorithm',
    [],
    [   [u'vector_Mat', u'blobs', '', ['/RW']],
        [u'String', u'name', '', []],
        [u'String', u'type', '', []],
        [u'int', u'preferableTarget', '', []]],
    None,
    u'@brief This interface class allows to build new Layers - are building blocks of networks.\n*\n* Each class, derived from Layer, must implement allocate() methods to declare own outputs and forward() to compute outputs.\n* Also before using the new layer into networks you must register your layer by using one of @ref dnnLayerFactory "LayerFactory" macros.']
docstring: @brief This interface class allows to build new Layers - are building blocks of networks.
*
* Each class, derived from Layer, must implement allocate() methods to declare own outputs and forward() to compute outputs.
* Also before using the new layer into networks you must register your layer by using one of @ref dnnLayerFactory "LayerFactory" macros.
ok: class CLASS cv.dnn::.Layer : Algorithm, name: Layer, base: Algorithm

--- Incoming ---
[   u'cv.dnn.Layer.finalize',
    u'void',
    [],
    [   [u'vector_Mat', u'inputs', u'', ['/C', '/Ref']],
        [u'vector_Mat', u'outputs', u'', ['/O', '/Ref']]],
    u'void',
    u'@brief @overload']
docstring: @brief @overload
ok: FUNC <void cv.dnn.Layer.finalize [ARG vector_Mat inputs=, ARG vector_Mat outputs=]>

--- Incoming ---
[   u'cv.dnn.Layer.finalize',
    u'vector_Mat',
    [],
    [[u'vector_Mat', u'inputs', u'', ['/C', '/Ref']]],
    u'std::vector<Mat>',
    u'@brief @overload']
docstring: @brief @overload
ok: FUNC <vector_Mat cv.dnn.Layer.finalize [ARG vector_Mat inputs=]>

--- Incoming ---
[   u'cv.dnn.Layer.run',
    u'void',
    [],
    [   [u'vector_Mat', u'inputs', u'', ['/C', '/Ref']],
        [u'vector_Mat', u'outputs', u'', ['/O', '/Ref']],
        [u'vector_Mat', u'internals', u'', ['/IO', '/Ref']]],
    u'void',
    u'@brief Allocates layer and computes output.']
docstring: @brief Allocates layer and computes output.
ok: FUNC <void cv.dnn.Layer.run [ARG vector_Mat inputs=, ARG vector_Mat outputs=, ARG vector_Mat internals=]>

--- Incoming ---
[   u'class cv.dnn.Net',
    '',
    ['/Simple'],
    [],
    None,
    u'@brief This class allows to create and manipulate comprehensive artificial neural networks.\n*\n* Neural network is presented as directed acyclic graph (DAG), where vertices are Layer instances,\n* and edges specify relationships between layers inputs and outputs.\n*\n* Each network layer has unique integer id and unique string name inside its network.\n* LayerId can store either layer name or layer id.\n*\n* This class supports reference counting of its instances, i. e. copies point to the same instance.']
docstring: @brief This class allows to create and manipulate comprehensive artificial neural networks.
*
* Neural network is presented as directed acyclic graph (DAG), where vertices are Layer instances,
* and edges specify relationships between layers inputs and outputs.
*
* Each network layer has unique integer id and unique string name inside its network.
* LayerId can store either layer name or layer id.
*
* This class supports reference counting of its instances, i. e. copies point to the same instance.
ok: class CLASS cv.dnn::.Net : , name: Net, base: 

--- Incoming ---
[u'cv.dnn.Net.Net', '', [], [], None, '']
ok: FUNC < cv.dnn.Net.Net []>

--- Incoming ---
[   u'cv.dnn.Net.empty',
    u'bool',
    ['/C'],
    [],
    u'bool',
    u'Returns true if there are no layers in the network.']
docstring: Returns true if there are no layers in the network.
ok: FUNC <bool cv.dnn.Net.empty []>

--- Incoming ---
[   u'cv.dnn.Net.getLayerId',
    u'int',
    [],
    [[u'String', u'layer', u'', ['/C', '/Ref']]],
    u'int',
    u"@brief Converts string name of the layer to the integer identifier.\n*  @returns id of the layer, or -1 if the layer wasn't found."]
docstring: @brief Converts string name of the layer to the integer identifier.
*  @returns id of the layer, or -1 if the layer wasn't found.
ok: FUNC <int cv.dnn.Net.getLayerId [ARG String layer=]>

--- Incoming ---
[   u'cv.dnn.Net.getLayerNames',
    u'vector_String',
    ['/C'],
    [],
    u'std::vector<String>',
    '']
ok: FUNC <vector_String cv.dnn.Net.getLayerNames []>

--- Incoming ---
[   u'cv.dnn.Net.getLayer',
    u'Ptr_Layer',
    [],
    [[u'LayerId', u'layerId', u'', []]],
    u'Ptr<Layer>',
    u'@brief Returns pointer to layer with specified id or name which the network use.']
docstring: @brief Returns pointer to layer with specified id or name which the network use.
ok: FUNC <Ptr_Layer cv.dnn.Net.getLayer [ARG LayerId layerId=]>

--- Incoming ---
[   u'cv.dnn.Net.deleteLayer',
    u'void',
    [],
    [[u'LayerId', u'layer', u'', []]],
    u'void',
    u'@brief Delete layer for the network (not implemented yet)']
docstring: @brief Delete layer for the network (not implemented yet)
ok: FUNC <void cv.dnn.Net.deleteLayer [ARG LayerId layer=]>

--- Incoming ---
[   u'cv.dnn.Net.connect',
    u'void',
    [],
    [[u'String', u'outPin', u'', []], [u'String', u'inpPin', u'', []]],
    u'void',
    u'@brief Connects output of the first layer to input of the second layer.\n*  @param outPin descriptor of the first layer output.\n*  @param inpPin descriptor of the second layer input.\n*\n* Descriptors have the following template <DFN>&lt;layer_name&gt;[.input_number]</DFN>:\n* - the first part of the template <DFN>layer_name</DFN> is sting name of the added layer.\n*   If this part is empty then the network input pseudo layer will be used;\n* - the second optional part of the template <DFN>input_number</DFN>\n*   is either number of the layer input, either label one.\n*   If this part is omitted then the first layer input will be used.\n*\n*  @see setNetInputs(), Layer::inputNameToIndex(), Layer::outputNameToIndex()']
docstring: @brief Connects output of the first layer to input of the second layer.
*  @param outPin descriptor of the first layer output.
*  @param inpPin descriptor of the second layer input.
*
* Descriptors have the following template <DFN>&lt;layer_name&gt;[.input_number]</DFN>:
* - the first part of the template <DFN>layer_name</DFN> is sting name of the added layer.
*   If this part is empty then the network input pseudo layer will be used;
* - the second optional part of the template <DFN>input_number</DFN>
*   is either number of the layer input, either label one.
*   If this part is omitted then the first layer input will be used.
*
*  @see setNetInputs(), Layer::inputNameToIndex(), Layer::outputNameToIndex()
ok: FUNC <void cv.dnn.Net.connect [ARG String outPin=, ARG String inpPin=]>

--- Incoming ---
[   u'cv.dnn.Net.setInputsNames',
    u'void',
    [],
    [[u'vector_String', u'inputBlobNames', u'', ['/C', '/Ref']]],
    u'void',
    u"@brief Sets outputs names of the network input pseudo layer.\n*\n* Each net always has special own the network input pseudo layer with id=0.\n* This layer stores the user blobs only and don't make any computations.\n* In fact, this layer provides the only way to pass user data into the network.\n* As any other layer, this layer can label its outputs and this function provides an easy way to do this."]
docstring: @brief Sets outputs names of the network input pseudo layer.
*
* Each net always has special own the network input pseudo layer with id=0.
* This layer stores the user blobs only and don't make any computations.
* In fact, this layer provides the only way to pass user data into the network.
* As any other layer, this layer can label its outputs and this function provides an easy way to do this.
ok: FUNC <void cv.dnn.Net.setInputsNames [ARG vector_String inputBlobNames=]>

--- Incoming ---
[   u'cv.dnn.Net.forward',
    u'Mat',
    [],
    [[u'String', u'outputName', u'String()', ['/C', '/Ref']]],
    u'Mat',
    u'@brief Runs forward pass to compute output of layer with name @p outputName.\n*  @param outputName name for layer which output is needed to get\n*  @return blob for first output of specified layer.\n*  @details By default runs forward pass for the whole network.']
docstring: @brief Runs forward pass to compute output of layer with name @p outputName.
*  @param outputName name for layer which output is needed to get
*  @return blob for first output of specified layer.
*  @details By default runs forward pass for the whole network.
ok: FUNC <Mat cv.dnn.Net.forward [ARG String outputName=String()]>

--- Incoming ---
[   u'cv.dnn.Net.forward',
    u'void',
    [],
    [   ['vector_Mat', u'outputBlobs', '', ['/O']],
        [u'String', u'outputName', u'String()', ['/C', '/Ref']]],
    u'void',
    u'@brief Runs forward pass to compute output of layer with name @p outputName.\n*  @param outputBlobs contains all output blobs for specified layer.\n*  @param outputName name for layer which output is needed to get\n*  @details If @p outputName is empty, runs forward pass for the whole network.']
docstring: @brief Runs forward pass to compute output of layer with name @p outputName.
*  @param outputBlobs contains all output blobs for specified layer.
*  @param outputName name for layer which output is needed to get
*  @details If @p outputName is empty, runs forward pass for the whole network.
ok: FUNC <void cv.dnn.Net.forward [ARG vector_Mat outputBlobs=, ARG String outputName=String()]>

--- Incoming ---
[   u'cv.dnn.Net.forward',
    u'void',
    [],
    [   ['vector_Mat', u'outputBlobs', '', ['/O']],
        [u'vector_String', u'outBlobNames', u'', ['/C', '/Ref']]],
    u'void',
    u'@brief Runs forward pass to compute outputs of layers listed in @p outBlobNames.\n*  @param outputBlobs contains blobs for first outputs of specified layers.\n*  @param outBlobNames names for layers which outputs are needed to get']
docstring: @brief Runs forward pass to compute outputs of layers listed in @p outBlobNames.
*  @param outputBlobs contains blobs for first outputs of specified layers.
*  @param outBlobNames names for layers which outputs are needed to get
ok: FUNC <void cv.dnn.Net.forward [ARG vector_Mat outputBlobs=, ARG vector_String outBlobNames=]>

--- Incoming ---
[   u'cv.dnn.Net.forward',
    u'void',
    [u'=forwardAndRetrieve'],
    [   [u'vector_vector_Mat', u'outputBlobs', u'', ['/O', '/Ref']],
        [u'vector_String', u'outBlobNames', u'', ['/C', '/Ref']]],
    u'void',
    u'@brief Runs forward pass to compute outputs of layers listed in @p outBlobNames.\n*  @param outputBlobs contains all output blobs for each layer specified in @p outBlobNames.\n*  @param outBlobNames names for layers which outputs are needed to get']
docstring: @brief Runs forward pass to compute outputs of layers listed in @p outBlobNames.
*  @param outputBlobs contains all output blobs for each layer specified in @p outBlobNames.
*  @param outBlobNames names for layers which outputs are needed to get
ok: FUNC <void cv.dnn.Net.forward [ARG vector_vector_Mat outputBlobs=, ARG vector_String outBlobNames=]>

--- Incoming ---
[   u'cv.dnn.Net.setHalideScheduler',
    u'void',
    [],
    [[u'String', u'scheduler', u'', ['/C', '/Ref']]],
    u'void',
    u'* @brief Compile Halide layers.\n* @param[in] scheduler Path to YAML file with scheduling directives.\n* @see setPreferableBackend\n*\n* Schedule layers that support Halide backend. Then compile them for\n* specific target. For layers that not represented in scheduling file\n* or if no manual scheduling used at all, automatic scheduling will be applied.']
docstring: * @brief Compile Halide layers.
* @param[in] scheduler Path to YAML file with scheduling directives.
* @see setPreferableBackend
*
* Schedule layers that support Halide backend. Then compile them for
* specific target. For layers that not represented in scheduling file
* or if no manual scheduling used at all, automatic scheduling will be applied.
ok: FUNC <void cv.dnn.Net.setHalideScheduler [ARG String scheduler=]>

--- Incoming ---
[   u'cv.dnn.Net.setPreferableBackend',
    u'void',
    [],
    [[u'int', u'backendId', u'', []]],
    u'void',
    u'* @brief Ask network to use specific computation backend where it supported.\n* @param[in] backendId backend identifier.\n* @see Backend']
docstring: * @brief Ask network to use specific computation backend where it supported.
* @param[in] backendId backend identifier.
* @see Backend
ok: FUNC <void cv.dnn.Net.setPreferableBackend [ARG int backendId=]>

--- Incoming ---
[   u'cv.dnn.Net.setPreferableTarget',
    u'void',
    [],
    [[u'int', u'targetId', u'', []]],
    u'void',
    u'* @brief Ask network to make computations on specific target device.\n* @param[in] targetId target identifier.\n* @see Target']
docstring: * @brief Ask network to make computations on specific target device.
* @param[in] targetId target identifier.
* @see Target
ok: FUNC <void cv.dnn.Net.setPreferableTarget [ARG int targetId=]>

--- Incoming ---
[   u'cv.dnn.Net.setInput',
    u'void',
    [],
    [['Mat', u'blob', '', []], [u'String', u'name', u'""', ['/C', '/Ref']]],
    u'void',
    u'@brief Sets the new value for the layer output blob\n*  @param name descriptor of the updating layer output blob.\n*  @param blob new blob.\n*  @see connect(String, String) to know format of the descriptor.\n*  @note If updating blob is not empty then @p blob must have the same shape,\n*  because network reshaping is not implemented yet.']
docstring: @brief Sets the new value for the layer output blob
*  @param name descriptor of the updating layer output blob.
*  @param blob new blob.
*  @see connect(String, String) to know format of the descriptor.
*  @note If updating blob is not empty then @p blob must have the same shape,
*  because network reshaping is not implemented yet.
ok: FUNC <void cv.dnn.Net.setInput [ARG Mat blob=, ARG String name=""]>

--- Incoming ---
[   u'cv.dnn.Net.setParam',
    u'void',
    [],
    [   [u'LayerId', u'layer', u'', []],
        [u'int', u'numParam', u'', []],
        [u'Mat', u'blob', u'', ['/C', '/Ref']]],
    u'void',
    u'@brief Sets the new value for the learned param of the layer.\n*  @param layer name or id of the layer.\n*  @param numParam index of the layer parameter in the Layer::blobs array.\n*  @param blob the new value.\n*  @see Layer::blobs\n*  @note If shape of the new blob differs from the previous shape,\n*  then the following forward pass may fail.']
docstring: @brief Sets the new value for the learned param of the layer.
*  @param layer name or id of the layer.
*  @param numParam index of the layer parameter in the Layer::blobs array.
*  @param blob the new value.
*  @see Layer::blobs
*  @note If shape of the new blob differs from the previous shape,
*  then the following forward pass may fail.
ok: FUNC <void cv.dnn.Net.setParam [ARG LayerId layer=, ARG int numParam=, ARG Mat blob=]>

--- Incoming ---
[   u'cv.dnn.Net.getParam',
    u'Mat',
    [],
    [[u'LayerId', u'layer', u'', []], [u'int', u'numParam', u'0', []]],
    u'Mat',
    u'@brief Returns parameter blob of the layer.\n*  @param layer name or id of the layer.\n*  @param numParam index of the layer parameter in the Layer::blobs array.\n*  @see Layer::blobs']
docstring: @brief Returns parameter blob of the layer.
*  @param layer name or id of the layer.
*  @param numParam index of the layer parameter in the Layer::blobs array.
*  @see Layer::blobs
ok: FUNC <Mat cv.dnn.Net.getParam [ARG LayerId layer=, ARG int numParam=0]>

--- Incoming ---
[   u'cv.dnn.Net.getUnconnectedOutLayers',
    u'vector_int',
    ['/C'],
    [],
    u'std::vector<int>',
    u'@brief Returns indexes of layers with unconnected outputs.']
docstring: @brief Returns indexes of layers with unconnected outputs.
ok: FUNC <vector_int cv.dnn.Net.getUnconnectedOutLayers []>

--- Incoming ---
[   u'cv.dnn.Net.getLayersShapes',
    u'void',
    ['/C'],
    [   [u'vector_MatShape', u'netInputShapes', u'', ['/C', '/Ref']],
        [u'vector_int', u'layersIds', u'', ['/O', '/Ref']],
        [u'vector_vector_MatShape', u'inLayersShapes', u'', ['/O', '/Ref']],
        [u'vector_vector_MatShape', u'outLayersShapes', u'', ['/O', '/Ref']]],
    u'void',
    u"@brief Returns input and output shapes for all layers in loaded model;\n*  preliminary inferencing isn't necessary.\n*  @param netInputShapes shapes for all input blobs in net input layer.\n*  @param layersIds output parameter for layer IDs.\n*  @param inLayersShapes output parameter for input layers shapes;\n* order is the same as in layersIds\n*  @param outLayersShapes output parameter for output layers shapes;\n* order is the same as in layersIds"]
docstring: @brief Returns input and output shapes for all layers in loaded model;
*  preliminary inferencing isn't necessary.
*  @param netInputShapes shapes for all input blobs in net input layer.
*  @param layersIds output parameter for layer IDs.
*  @param inLayersShapes output parameter for input layers shapes;
* order is the same as in layersIds
*  @param outLayersShapes output parameter for output layers shapes;
* order is the same as in layersIds
ok: FUNC <void cv.dnn.Net.getLayersShapes [ARG vector_MatShape netInputShapes=, ARG vector_int layersIds=, ARG vector_vector_MatShape inLayersShapes=, ARG vector_vector_MatShape outLayersShapes=]>

--- Incoming ---
[   u'cv.dnn.Net.getLayersShapes',
    u'void',
    ['/C'],
    [   [u'MatShape', u'netInputShape', u'', ['/C', '/Ref']],
        [u'vector_int', u'layersIds', u'', ['/O', '/Ref']],
        [u'vector_vector_MatShape', u'inLayersShapes', u'', ['/O', '/Ref']],
        [u'vector_vector_MatShape', u'outLayersShapes', u'', ['/O', '/Ref']]],
    u'void',
    u'@overload']
docstring: @overload
ok: FUNC <void cv.dnn.Net.getLayersShapes [ARG MatShape netInputShape=, ARG vector_int layersIds=, ARG vector_vector_MatShape inLayersShapes=, ARG vector_vector_MatShape outLayersShapes=]>

--- Incoming ---
[   u'cv.dnn.Net.getFLOPS',
    u'int64',
    ['/C'],
    [[u'vector_MatShape', u'netInputShapes', u'', ['/C', '/Ref']]],
    u'int64',
    u'@brief Computes FLOP for whole loaded model with specified input shapes.\n* @param netInputShapes vector of shapes for all net inputs.\n* @returns computed FLOP.']
docstring: @brief Computes FLOP for whole loaded model with specified input shapes.
* @param netInputShapes vector of shapes for all net inputs.
* @returns computed FLOP.
ok: FUNC <int64 cv.dnn.Net.getFLOPS [ARG vector_MatShape netInputShapes=]>

--- Incoming ---
[   u'cv.dnn.Net.getFLOPS',
    u'int64',
    ['/C'],
    [[u'MatShape', u'netInputShape', u'', ['/C', '/Ref']]],
    u'int64',
    u'@overload']
docstring: @overload
ok: FUNC <int64 cv.dnn.Net.getFLOPS [ARG MatShape netInputShape=]>

--- Incoming ---
[   u'cv.dnn.Net.getFLOPS',
    u'int64',
    ['/C'],
    [   [u'int', u'layerId', u'', ['/C']],
        [u'vector_MatShape', u'netInputShapes', u'', ['/C', '/Ref']]],
    u'int64',
    u'@overload']
docstring: @overload
ok: FUNC <int64 cv.dnn.Net.getFLOPS [ARG int layerId=, ARG vector_MatShape netInputShapes=]>

--- Incoming ---
[   u'cv.dnn.Net.getFLOPS',
    u'int64',
    ['/C'],
    [   [u'int', u'layerId', u'', ['/C']],
        [u'MatShape', u'netInputShape', u'', ['/C', '/Ref']]],
    u'int64',
    u'@overload']
docstring: @overload
ok: FUNC <int64 cv.dnn.Net.getFLOPS [ARG int layerId=, ARG MatShape netInputShape=]>

--- Incoming ---
[   u'cv.dnn.Net.getLayerTypes',
    u'void',
    ['/C'],
    [[u'vector_String', u'layersTypes', u'', ['/O', '/Ref']]],
    u'void',
    u'@brief Returns list of types for layer used in model.\n* @param layersTypes output parameter for returning types.']
docstring: @brief Returns list of types for layer used in model.
* @param layersTypes output parameter for returning types.
ok: FUNC <void cv.dnn.Net.getLayerTypes [ARG vector_String layersTypes=]>

--- Incoming ---
[   u'cv.dnn.Net.getLayersCount',
    u'int',
    ['/C'],
    [[u'String', u'layerType', u'', ['/C', '/Ref']]],
    u'int',
    u'@brief Returns count of layers of specified type.\n* @param layerType type.\n* @returns count of layers']
docstring: @brief Returns count of layers of specified type.
* @param layerType type.
* @returns count of layers
ok: FUNC <int cv.dnn.Net.getLayersCount [ARG String layerType=]>

--- Incoming ---
[   u'cv.dnn.Net.getMemoryConsumption',
    u'void',
    ['/C'],
    [   [u'MatShape', u'netInputShape', u'', ['/C', '/Ref']],
        [u'size_t', u'weights', u'', ['/O', '/Ref']],
        [u'size_t', u'blobs', u'', ['/O', '/Ref']]],
    u'void',
    u'@overload']
docstring: @overload
ok: FUNC <void cv.dnn.Net.getMemoryConsumption [ARG MatShape netInputShape=, ARG size_t weights=, ARG size_t blobs=]>

--- Incoming ---
[   u'cv.dnn.Net.getMemoryConsumption',
    u'void',
    ['/C'],
    [   [u'int', u'layerId', u'', ['/C']],
        [u'vector_MatShape', u'netInputShapes', u'', ['/C', '/Ref']],
        [u'size_t', u'weights', u'', ['/O', '/Ref']],
        [u'size_t', u'blobs', u'', ['/O', '/Ref']]],
    u'void',
    u'@overload']
docstring: @overload
ok: FUNC <void cv.dnn.Net.getMemoryConsumption [ARG int layerId=, ARG vector_MatShape netInputShapes=, ARG size_t weights=, ARG size_t blobs=]>

--- Incoming ---
[   u'cv.dnn.Net.getMemoryConsumption',
    u'void',
    ['/C'],
    [   [u'int', u'layerId', u'', ['/C']],
        [u'MatShape', u'netInputShape', u'', ['/C', '/Ref']],
        [u'size_t', u'weights', u'', ['/O', '/Ref']],
        [u'size_t', u'blobs', u'', ['/O', '/Ref']]],
    u'void',
    u'@overload']
docstring: @overload
ok: FUNC <void cv.dnn.Net.getMemoryConsumption [ARG int layerId=, ARG MatShape netInputShape=, ARG size_t weights=, ARG size_t blobs=]>

--- Incoming ---
[   u'cv.dnn.Net.enableFusion',
    u'void',
    [],
    [[u'bool', u'fusion', u'', []]],
    u'void',
    u'@brief Enables or disables layer fusion in the network.\n* @param fusion true to enable the fusion, false to disable. The fusion is enabled by default.']
docstring: @brief Enables or disables layer fusion in the network.
* @param fusion true to enable the fusion, false to disable. The fusion is enabled by default.
ok: FUNC <void cv.dnn.Net.enableFusion [ARG bool fusion=]>

--- Incoming ---
[   u'cv.dnn.Net.getPerfProfile',
    u'int64',
    [],
    [[u'vector_double', u'timings', u'', ['/O', '/Ref']]],
    u'int64',
    u'@brief Returns overall time for inference and timings (in ticks) for layers.\n* Indexes in returned vector correspond to layers ids. Some layers can be fused with others,\n* in this case zero ticks count will be return for that skipped layers.\n* @param timings vector for tick timings for all layers.\n* @return overall ticks for model inference.']
docstring: @brief Returns overall time for inference and timings (in ticks) for layers.
* Indexes in returned vector correspond to layers ids. Some layers can be fused with others,
* in this case zero ticks count will be return for that skipped layers.
* @param timings vector for tick timings for all layers.
* @return overall ticks for model inference.
ok: FUNC <int64 cv.dnn.Net.getPerfProfile [ARG vector_double timings=]>

--- Incoming ---
[   u'cv.dnn.readNetFromDarknet',
    u'Net',
    [],
    [   [u'String', u'cfgFile', u'', ['/C', '/Ref']],
        [u'String', u'darknetModel', u'String()', ['/C', '/Ref']]],
    u'Net',
    u'@brief Reads a network model stored in <a href="https://pjreddie.com/darknet/">Darknet</a> model files.\n*  @param cfgFile      path to the .cfg file with text description of the network architecture.\n*  @param darknetModel path to the .weights file with learned network.\n*  @returns Network object that ready to do forward, throw an exception in failure cases.\n*  @returns Net object.']
docstring: @brief Reads a network model stored in <a href="https://pjreddie.com/darknet/">Darknet</a> model files.
*  @param cfgFile      path to the .cfg file with text description of the network architecture.
*  @param darknetModel path to the .weights file with learned network.
*  @returns Network object that ready to do forward, throw an exception in failure cases.
*  @returns Net object.
ok: FUNC <Net cv.dnn..readNetFromDarknet [ARG String cfgFile=, ARG String darknetModel=String()]>

--- Incoming ---
[   u'cv.dnn.readNetFromCaffe',
    u'Net',
    [],
    [   [u'String', u'prototxt', u'', ['/C', '/Ref']],
        [u'String', u'caffeModel', u'String()', ['/C', '/Ref']]],
    u'Net',
    u'@brief Reads a network model stored in <a href="http://caffe.berkeleyvision.org">Caffe</a> framework\'s format.\n* @param prototxt   path to the .prototxt file with text description of the network architecture.\n* @param caffeModel path to the .caffemodel file with learned network.\n* @returns Net object.']
docstring: @brief Reads a network model stored in <a href="http://caffe.berkeleyvision.org">Caffe</a> framework's format.
* @param prototxt   path to the .prototxt file with text description of the network architecture.
* @param caffeModel path to the .caffemodel file with learned network.
* @returns Net object.
ok: FUNC <Net cv.dnn..readNetFromCaffe [ARG String prototxt=, ARG String caffeModel=String()]>

--- Incoming ---
[   u'cv.dnn.readNetFromTensorflow',
    u'Net',
    [],
    [   [u'String', u'model', u'', ['/C', '/Ref']],
        [u'String', u'config', u'String()', ['/C', '/Ref']]],
    u'Net',
    u'@brief Reads a network model stored in <a href="https://www.tensorflow.org/">TensorFlow</a> framework\'s format.\n* @param model  path to the .pb file with binary protobuf description of the network architecture\n* @param config path to the .pbtxt file that contains text graph definition in protobuf format.\n*               Resulting Net object is built by text graph using weights from a binary one that\n*               let us make it more flexible.\n* @returns Net object.']
docstring: @brief Reads a network model stored in <a href="https://www.tensorflow.org/">TensorFlow</a> framework's format.
* @param model  path to the .pb file with binary protobuf description of the network architecture
* @param config path to the .pbtxt file that contains text graph definition in protobuf format.
*               Resulting Net object is built by text graph using weights from a binary one that
*               let us make it more flexible.
* @returns Net object.
ok: FUNC <Net cv.dnn..readNetFromTensorflow [ARG String model=, ARG String config=String()]>

--- Incoming ---
[   u'cv.dnn.readNetFromTorch',
    u'Net',
    [],
    [   [u'String', u'model', u'', ['/C', '/Ref']],
        [u'bool', u'isBinary', u'true', []]],
    u'Net',
    u'*  @brief Reads a network model stored in <a href="http://torch.ch">Torch7</a> framework\'s format.\n*  @param model    path to the file, dumped from Torch by using torch.save() function.\n*  @param isBinary specifies whether the network was serialized in ascii mode or binary.\n*  @returns Net object.\n*\n*  @note Ascii mode of Torch serializer is more preferable, because binary mode extensively use `long` type of C language,\n*  which has various bit-length on different systems.\n*\n* The loading file must contain serialized <a href="https://github.com/torch/nn/blob/master/doc/module.md">nn.Module</a> object\n* with importing network. Try to eliminate a custom objects from serialazing data to avoid importing errors.\n*\n* List of supported layers (i.e. object instances derived from Torch nn.Module class):\n* - nn.Sequential\n* - nn.Parallel\n* - nn.Concat\n* - nn.Linear\n* - nn.SpatialConvolution\n* - nn.SpatialMaxPooling, nn.SpatialAveragePooling\n* - nn.ReLU, nn.TanH, nn.Sigmoid\n* - nn.Reshape\n* - nn.SoftMax, nn.LogSoftMax\n*\n* Also some equivalents of these classes from cunn, cudnn, and fbcunn may be successfully imported.']
docstring: *  @brief Reads a network model stored in <a href="http://torch.ch">Torch7</a> framework's format.
*  @param model    path to the file, dumped from Torch by using torch.save() function.
*  @param isBinary specifies whether the network was serialized in ascii mode or binary.
*  @returns Net object.
*
*  @note Ascii mode of Torch serializer is more preferable, because binary mode extensively use `long` type of C language,
*  which has various bit-length on different systems.
*
* The loading file must contain serialized <a href="https://github.com/torch/nn/blob/master/doc/module.md">nn.Module</a> object
* with importing network. Try to eliminate a custom objects from serialazing data to avoid importing errors.
*
* List of supported layers (i.e. object instances derived from Torch nn.Module class):
* - nn.Sequential
* - nn.Parallel
* - nn.Concat
* - nn.Linear
* - nn.SpatialConvolution
* - nn.SpatialMaxPooling, nn.SpatialAveragePooling
* - nn.ReLU, nn.TanH, nn.Sigmoid
* - nn.Reshape
* - nn.SoftMax, nn.LogSoftMax
*
* Also some equivalents of these classes from cunn, cudnn, and fbcunn may be successfully imported.
ok: FUNC <Net cv.dnn..readNetFromTorch [ARG String model=, ARG bool isBinary=true]>

--- Incoming ---
[   u'cv.dnn.readTorchBlob',
    u'Mat',
    [],
    [   [u'String', u'filename', u'', ['/C', '/Ref']],
        [u'bool', u'isBinary', u'true', []]],
    u'Mat',
    u'@brief Loads blob which was serialized as torch.Tensor object of Torch7 framework.\n*  @warning This function has the same limitations as readNetFromTorch().']
docstring: @brief Loads blob which was serialized as torch.Tensor object of Torch7 framework.
*  @warning This function has the same limitations as readNetFromTorch().
ok: FUNC <Mat cv.dnn..readTorchBlob [ARG String filename=, ARG bool isBinary=true]>

--- Incoming ---
[   u'cv.dnn.blobFromImage',
    u'Mat',
    [],
    [   ['Mat', u'image', '', []],
        [u'double', u'scalefactor', u'1.0', []],
        [u'Size', u'size', u'Size()', ['/C', '/Ref']],
        [u'Scalar', u'mean', u'Scalar()', ['/C', '/Ref']],
        [u'bool', u'swapRB', u'true', []],
        [u'bool', u'crop', u'true', []]],
    u'Mat',
    u'@brief Creates 4-dimensional blob from image. Optionally resizes and crops @p image from center,\n*  subtract @p mean values, scales values by @p scalefactor, swap Blue and Red channels.\n*  @param image input image (with 1-, 3- or 4-channels).\n*  @param size spatial size for output image\n*  @param mean scalar with mean values which are subtracted from channels. Values are intended\n*  to be in (mean-R, mean-G, mean-B) order if @p image has BGR ordering and @p swapRB is true.\n*  @param scalefactor multiplier for @p image values.\n*  @param swapRB flag which indicates that swap first and last channels\n*  in 3-channel image is necessary.\n*  @param crop flag which indicates whether image will be cropped after resize or not\n*  @details if @p crop is true, input image is resized so one side after resize is equal to corresponing\n*  dimension in @p size and another one is equal or larger. Then, crop from the center is performed.\n*  If @p crop is false, direct resize without cropping and preserving aspect ratio is performed.\n*  @returns 4-dimansional Mat with NCHW dimensions order.']
docstring: @brief Creates 4-dimensional blob from image. Optionally resizes and crops @p image from center,
*  subtract @p mean values, scales values by @p scalefactor, swap Blue and Red channels.
*  @param image input image (with 1-, 3- or 4-channels).
*  @param size spatial size for output image
*  @param mean scalar with mean values which are subtracted from channels. Values are intended
*  to be in (mean-R, mean-G, mean-B) order if @p image has BGR ordering and @p swapRB is true.
*  @param scalefactor multiplier for @p image values.
*  @param swapRB flag which indicates that swap first and last channels
*  in 3-channel image is necessary.
*  @param crop flag which indicates whether image will be cropped after resize or not
*  @details if @p crop is true, input image is resized so one side after resize is equal to corresponing
*  dimension in @p size and another one is equal or larger. Then, crop from the center is performed.
*  If @p crop is false, direct resize without cropping and preserving aspect ratio is performed.
*  @returns 4-dimansional Mat with NCHW dimensions order.
ok: FUNC <Mat cv.dnn..blobFromImage [ARG Mat image=, ARG double scalefactor=1.0, ARG Size size=Size(), ARG Scalar mean=Scalar(), ARG bool swapRB=true, ARG bool crop=true]>

--- Incoming ---
[   u'cv.dnn.blobFromImages',
    u'Mat',
    [],
    [   [u'vector_Mat', u'images', u'', ['/C', '/Ref']],
        [u'double', u'scalefactor', u'1.0', []],
        [u'Size', u'size', u'Size()', []],
        [u'Scalar', u'mean', u'Scalar()', ['/C', '/Ref']],
        [u'bool', u'swapRB', u'true', []],
        [u'bool', u'crop', u'true', []]],
    u'Mat',
    u'@brief Creates 4-dimensional blob from series of images. Optionally resizes and\n*  crops @p images from center, subtract @p mean values, scales values by @p scalefactor,\n*  swap Blue and Red channels.\n*  @param images input images (all with 1-, 3- or 4-channels).\n*  @param size spatial size for output image\n*  @param mean scalar with mean values which are subtracted from channels. Values are intended\n*  to be in (mean-R, mean-G, mean-B) order if @p image has BGR ordering and @p swapRB is true.\n*  @param scalefactor multiplier for @p images values.\n*  @param swapRB flag which indicates that swap first and last channels\n*  in 3-channel image is necessary.\n*  @param crop flag which indicates whether image will be cropped after resize or not\n*  @details if @p crop is true, input image is resized so one side after resize is equal to corresponing\n*  dimension in @p size and another one is equal or larger. Then, crop from the center is performed.\n*  If @p crop is false, direct resize without cropping and preserving aspect ratio is performed.\n*  @returns 4-dimansional Mat with NCHW dimensions order.']
docstring: @brief Creates 4-dimensional blob from series of images. Optionally resizes and
*  crops @p images from center, subtract @p mean values, scales values by @p scalefactor,
*  swap Blue and Red channels.
*  @param images input images (all with 1-, 3- or 4-channels).
*  @param size spatial size for output image
*  @param mean scalar with mean values which are subtracted from channels. Values are intended
*  to be in (mean-R, mean-G, mean-B) order if @p image has BGR ordering and @p swapRB is true.
*  @param scalefactor multiplier for @p images values.
*  @param swapRB flag which indicates that swap first and last channels
*  in 3-channel image is necessary.
*  @param crop flag which indicates whether image will be cropped after resize or not
*  @details if @p crop is true, input image is resized so one side after resize is equal to corresponing
*  dimension in @p size and another one is equal or larger. Then, crop from the center is performed.
*  If @p crop is false, direct resize without cropping and preserving aspect ratio is performed.
*  @returns 4-dimansional Mat with NCHW dimensions order.
ok: FUNC <Mat cv.dnn..blobFromImages [ARG vector_Mat images=, ARG double scalefactor=1.0, ARG Size size=Size(), ARG Scalar mean=Scalar(), ARG bool swapRB=true, ARG bool crop=true]>

--- Incoming ---
[   u'cv.dnn.shrinkCaffeModel',
    u'void',
    [],
    [   [u'String', u'src', u'', ['/C', '/Ref']],
        [u'String', u'dst', u'', ['/C', '/Ref']],
        [   u'vector_String',
            u'layersTypes',
            u'std::vector<String>()',
            ['/C', '/Ref']]],
    u'void',
    u"@brief Convert all weights of Caffe network to half precision floating point.\n* @param src Path to origin model from Caffe framework contains single\n*            precision floating point weights (usually has `.caffemodel` extension).\n* @param dst Path to destination model with updated weights.\n* @param layersTypes Set of layers types which parameters will be converted.\n*                    By default, converts only Convolutional and Fully-Connected layers'\n*                    weights.\n*\n* @note Shrinked model has no origin float32 weights so it can't be used\n*       in origin Caffe framework anymore. However the structure of data\n*       is taken from NVidia's Caffe fork: https://github.com/NVIDIA/caffe.\n*       So the resulting model may be used there."]
docstring: @brief Convert all weights of Caffe network to half precision floating point.
* @param src Path to origin model from Caffe framework contains single
*            precision floating point weights (usually has `.caffemodel` extension).
* @param dst Path to destination model with updated weights.
* @param layersTypes Set of layers types which parameters will be converted.
*                    By default, converts only Convolutional and Fully-Connected layers'
*                    weights.
*
* @note Shrinked model has no origin float32 weights so it can't be used
*       in origin Caffe framework anymore. However the structure of data
*       is taken from NVidia's Caffe fork: https://github.com/NVIDIA/caffe.
*       So the resulting model may be used there.
ok: FUNC <void cv.dnn..shrinkCaffeModel [ARG String src=, ARG String dst=, ARG vector_String layersTypes=std::vector<String>()]>

--- Incoming ---
[   u'cv.dnn.NMSBoxes',
    u'void',
    [],
    [   [u'vector_Rect', u'bboxes', u'', ['/C', '/Ref']],
        [u'vector_float', u'scores', u'', ['/C', '/Ref']],
        [u'float', u'score_threshold', u'', ['/C']],
        [u'float', u'nms_threshold', u'', ['/C']],
        [u'vector_int', u'indices', u'', ['/O', '/Ref']],
        [u'float', u'eta', u'1.f', ['/C']],
        [u'int', u'top_k', u'0', ['/C']]],
    u'void',
    u'@brief Performs non maximum suppression given boxes and corresponding scores.\n\n* @param bboxes a set of bounding boxes to apply NMS.\n* @param scores a set of corresponding confidences.\n* @param score_threshold a threshold used to filter boxes by score.\n* @param nms_threshold a threshold used in non maximum suppression.\n* @param indices the kept indices of bboxes after NMS.\n* @param eta a coefficient in adaptive threshold formula: \\f$nms\\_threshold_{i+1}=eta\\cdot nms\\_threshold_i\\f$.\n* @param top_k if `>0`, keep at most @p top_k picked indices.']
docstring: @brief Performs non maximum suppression given boxes and corresponding scores.

* @param bboxes a set of bounding boxes to apply NMS.
* @param scores a set of corresponding confidences.
* @param score_threshold a threshold used to filter boxes by score.
* @param nms_threshold a threshold used in non maximum suppression.
* @param indices the kept indices of bboxes after NMS.
* @param eta a coefficient in adaptive threshold formula: \f$nms\_threshold_{i+1}=eta\cdot nms\_threshold_i\f$.
* @param top_k if `>0`, keep at most @p top_k picked indices.
ok: FUNC <void cv.dnn..NMSBoxes [ARG vector_Rect bboxes=, ARG vector_float scores=, ARG float score_threshold=, ARG float nms_threshold=, ARG vector_int indices=, ARG float eta=1.f, ARG int top_k=0]>


===== Header: /home/jeon/다운로드/opencv-3.4.0/modules/dnn/include/opencv2/dnn/dict.hpp =====
Namespaces: set([u'cv.dnn.<unnamed>', u'cv.dnn.experimental_dnn_v3', u'cv.dnn', u'cv', u'cv.dnn.details'])

--- Incoming ---
[   u'struct cv.dnn.DictValue',
    '',
    [],
    [],
    None,
    u'@brief This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64.\n*  @todo Maybe int64 is useless because double type exactly stores at least 2^52 integers.']
docstring: @brief This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64.
*  @todo Maybe int64 is useless because double type exactly stores at least 2^52 integers.
ok: class CLASS cv.dnn::.DictValue : , name: DictValue, base: 

--- Incoming ---
[u'cv.dnn.DictValue.DictValue', '', [], [[u'int', u'i', u'', []]], None, '']
ok: FUNC < cv.dnn.DictValue.DictValue [ARG int i=]>

--- Incoming ---
[u'cv.dnn.DictValue.DictValue', '', [], [[u'double', u'p', u'', []]], None, '']
ok: FUNC < cv.dnn.DictValue.DictValue [ARG double p=]>

--- Incoming ---
[   u'cv.dnn.DictValue.DictValue',
    '',
    [],
    [[u'String', u's', u'', ['/C', '/Ref']]],
    None,
    '']
ok: FUNC < cv.dnn.DictValue.DictValue [ARG String s=]>

--- Incoming ---
[u'cv.dnn.DictValue.isInt', u'bool', ['/C'], [], u'bool', '']
ok: FUNC <bool cv.dnn.DictValue.isInt []>

--- Incoming ---
[u'cv.dnn.DictValue.isString', u'bool', ['/C'], [], u'bool', '']
ok: FUNC <bool cv.dnn.DictValue.isString []>

--- Incoming ---
[u'cv.dnn.DictValue.isReal', u'bool', ['/C'], [], u'bool', '']
ok: FUNC <bool cv.dnn.DictValue.isReal []>

--- Incoming ---
[   u'cv.dnn.DictValue.getIntValue',
    u'int',
    ['/C'],
    [[u'int', u'idx', u'-1', []]],
    u'int',
    '']
ok: FUNC <int cv.dnn.DictValue.getIntValue [ARG int idx=-1]>

--- Incoming ---
[   u'cv.dnn.DictValue.getRealValue',
    u'double',
    ['/C'],
    [[u'int', u'idx', u'-1', []]],
    u'double',
    '']
ok: FUNC <double cv.dnn.DictValue.getRealValue [ARG int idx=-1]>

--- Incoming ---
[   u'cv.dnn.DictValue.getStringValue',
    u'String',
    ['/C'],
    [[u'int', u'idx', u'-1', []]],
    u'String',
    '']
ok: FUNC <String cv.dnn.DictValue.getStringValue [ARG int idx=-1]>


===== Header: /home/jeon/다운로드/opencv-3.4.0/modules/dnn/include/opencv2/dnn/layer.hpp =====
Namespaces: set([u'cv.dnn.<unnamed>', u'cv.dnn.experimental_dnn_v3', u'cv.dnn', u'cv', u'cv.dnn.details'])
Ignore header: /home/jeon/다운로드/opencv-3.4.0/modules/dnn/include/opencv2/dnn/layer.hpp


===== Header: /home/jeon/다운로드/opencv-3.4.0/modules/dnn/include/opencv2/dnn/all_layers.hpp =====
Namespaces: set([u'cv.dnn.<unnamed>', u'cv.dnn.experimental_dnn_v3', u'cv.dnn', u'cv', u'cv.dnn.details'])
Ignore header: /home/jeon/다운로드/opencv-3.4.0/modules/dnn/include/opencv2/dnn/all_layers.hpp


===== Generating... =====
CLASS cv.dnn::.Layer : Algorithm
FUNC <vector_Mat cv.dnn.Layer.finalize [ARG vector_Mat inputs=]>
java: List<Mat> finalize(List<Mat> inputs)
FUNC <void cv.dnn.Layer.finalize [ARG vector_Mat inputs=, ARG vector_Mat outputs=]>
java: void finalize(List<Mat> inputs, List<Mat> outputs)
FUNC <void cv.dnn.Layer.run [ARG vector_Mat inputs=, ARG vector_Mat outputs=, ARG vector_Mat internals=]>
java: void run(List<Mat> inputs, List<Mat> outputs, List<Mat> internals)
FUNC <vector_Mat cv.dnn.Layer.get_blobs []>
java: List<Mat> get_blobs()
FUNC <void cv.dnn.Layer.set_blobs [ARG vector_Mat blobs=]>
java: void set_blobs(List<Mat> blobs)
FUNC <String cv.dnn.Layer.get_name []>
java: String get_name()
FUNC <String cv.dnn.Layer.get_type []>
java: String get_type()
FUNC <int cv.dnn.Layer.get_preferableTarget []>
java: int get_preferableTarget()
CLASS ::.Dnn : 
[CONST DNN_BACKEND_DEFAULT=0, CONST DNN_BACKEND_HALIDE=1, CONST DNN_TARGET_CPU=0, CONST DNN_TARGET_OPENCL=1]
FUNC <Mat cv.dnn..blobFromImage [ARG Mat image=, ARG double scalefactor=1.0, ARG Size size=Size(), ARG Scalar mean=Scalar(), ARG bool swapRB=true, ARG bool crop=true]>
java: Mat blobFromImage(Mat image, double scalefactor, Size size, Scalar mean, boolean swapRB, boolean crop)
java: Mat blobFromImage(Mat image)
FUNC <Mat cv.dnn..blobFromImages [ARG vector_Mat images=, ARG double scalefactor=1.0, ARG Size size=Size(), ARG Scalar mean=Scalar(), ARG bool swapRB=true, ARG bool crop=true]>
java: Mat blobFromImages(List<Mat> images, double scalefactor, Size size, Scalar mean, boolean swapRB, boolean crop)
java: Mat blobFromImages(List<Mat> images)
FUNC <Mat cv.dnn..readTorchBlob [ARG String filename=, ARG bool isBinary=true]>
java: Mat readTorchBlob(String filename, boolean isBinary)
java: Mat readTorchBlob(String filename)
FUNC <Net cv.dnn..readNetFromCaffe [ARG String prototxt=, ARG String caffeModel=String()]>
java: Net readNetFromCaffe(String prototxt, String caffeModel)
java: Net readNetFromCaffe(String prototxt)
FUNC <Net cv.dnn..readNetFromDarknet [ARG String cfgFile=, ARG String darknetModel=String()]>
java: Net readNetFromDarknet(String cfgFile, String darknetModel)
java: Net readNetFromDarknet(String cfgFile)
FUNC <Net cv.dnn..readNetFromTensorflow [ARG String model=, ARG String config=String()]>
java: Net readNetFromTensorflow(String model, String config)
java: Net readNetFromTensorflow(String model)
FUNC <Net cv.dnn..readNetFromTorch [ARG String model=, ARG bool isBinary=true]>
java: Net readNetFromTorch(String model, boolean isBinary)
java: Net readNetFromTorch(String model)
FUNC <void cv.dnn..NMSBoxes [ARG vector_Rect bboxes=, ARG vector_float scores=, ARG float score_threshold=, ARG float nms_threshold=, ARG vector_int indices=, ARG float eta=1.f, ARG int top_k=0]>
java: void NMSBoxes(MatOfRect bboxes, MatOfFloat scores, float score_threshold, float nms_threshold, MatOfInt indices, float eta, int top_k)
java: void NMSBoxes(MatOfRect bboxes, MatOfFloat scores, float score_threshold, float nms_threshold, MatOfInt indices)
FUNC <void cv.dnn..shrinkCaffeModel [ARG String src=, ARG String dst=, ARG vector_String layersTypes=std::vector<String>()]>
java: void shrinkCaffeModel(String src, String dst, List<String> layersTypes)
java: void shrinkCaffeModel(String src, String dst)
CLASS cv.dnn::.Net : 
FUNC < cv.dnn.Net.Net []>
java:  Net()
FUNC <Mat cv.dnn.Net.forward [ARG String outputName=String()]>
java: Mat forward(String outputName)
java: Mat forward()
FUNC <Mat cv.dnn.Net.getParam [ARG LayerId layer=, ARG int numParam=0]>
java: Mat getParam(DictValue layer, int numParam)
java: Mat getParam(DictValue layer)
FUNC <Ptr_Layer cv.dnn.Net.getLayer [ARG LayerId layerId=]>
java: Layer getLayer(DictValue layerId)
FUNC <bool cv.dnn.Net.empty []>
java: boolean empty()
FUNC <int cv.dnn.Net.getLayerId [ARG String layer=]>
java: int getLayerId(String layer)
FUNC <int cv.dnn.Net.getLayersCount [ARG String layerType=]>
java: int getLayersCount(String layerType)
FUNC <int64 cv.dnn.Net.getFLOPS [ARG MatShape netInputShape=]>
java: long getFLOPS(MatOfInt netInputShape)
FUNC <int64 cv.dnn.Net.getFLOPS [ARG int layerId=, ARG MatShape netInputShape=]>
java: long getFLOPS(int layerId, MatOfInt netInputShape)
FUNC <int64 cv.dnn.Net.getFLOPS [ARG int layerId=, ARG vector_MatShape netInputShapes=]>
java: long getFLOPS(int layerId, List<MatOfInt> netInputShapes)
FUNC <int64 cv.dnn.Net.getFLOPS [ARG vector_MatShape netInputShapes=]>
java: long getFLOPS(List<MatOfInt> netInputShapes)
FUNC <int64 cv.dnn.Net.getPerfProfile [ARG vector_double timings=]>
java: long getPerfProfile(MatOfDouble timings)
FUNC <vector_String cv.dnn.Net.getLayerNames []>
java: List<String> getLayerNames()
FUNC <vector_int cv.dnn.Net.getUnconnectedOutLayers []>
java: MatOfInt getUnconnectedOutLayers()
FUNC <void cv.dnn.Net.connect [ARG String outPin=, ARG String inpPin=]>
java: void connect(String outPin, String inpPin)
FUNC <void cv.dnn.Net.deleteLayer [ARG LayerId layer=]>
java: void deleteLayer(DictValue layer)
FUNC <void cv.dnn.Net.enableFusion [ARG bool fusion=]>
java: void enableFusion(boolean fusion)
FUNC <void cv.dnn.Net.forward [ARG vector_Mat outputBlobs=, ARG String outputName=String()]>
java: void forward(List<Mat> outputBlobs, String outputName)
java: void forward(List<Mat> outputBlobs)
FUNC <void cv.dnn.Net.forward [ARG vector_Mat outputBlobs=, ARG vector_String outBlobNames=]>
java: void forward(List<Mat> outputBlobs, List<String> outBlobNames)
FUNC <void cv.dnn.Net.forward [ARG vector_vector_Mat outputBlobs=, ARG vector_String outBlobNames=]>
SKIP:void forward(vector_vector_Mat& outputBlobs, vector_String outBlobNames)	 due to ARG typevector_vector_Mat/O
FUNC <void cv.dnn.Net.getLayerTypes [ARG vector_String layersTypes=]>
java: void getLayerTypes(List<String> layersTypes)
FUNC <void cv.dnn.Net.getLayersShapes [ARG MatShape netInputShape=, ARG vector_int layersIds=, ARG vector_vector_MatShape inLayersShapes=, ARG vector_vector_MatShape outLayersShapes=]>
SKIP:void getLayersShapes(MatShape netInputShape, vector_int& layersIds, vector_vector_MatShape& inLayersShapes, vector_vector_MatShape& outLayersShapes)	 due to ARG typevector_vector_MatShape/O
FUNC <void cv.dnn.Net.getLayersShapes [ARG vector_MatShape netInputShapes=, ARG vector_int layersIds=, ARG vector_vector_MatShape inLayersShapes=, ARG vector_vector_MatShape outLayersShapes=]>
SKIP:void getLayersShapes(vector_MatShape netInputShapes, vector_int& layersIds, vector_vector_MatShape& inLayersShapes, vector_vector_MatShape& outLayersShapes)	 due to ARG typevector_vector_MatShape/O
FUNC <void cv.dnn.Net.getMemoryConsumption [ARG MatShape netInputShape=, ARG size_t weights=, ARG size_t blobs=]>
java: void getMemoryConsumption(MatOfInt netInputShape, long[] weights, long[] blobs)
FUNC <void cv.dnn.Net.getMemoryConsumption [ARG int layerId=, ARG MatShape netInputShape=, ARG size_t weights=, ARG size_t blobs=]>
java: void getMemoryConsumption(int layerId, MatOfInt netInputShape, long[] weights, long[] blobs)
FUNC <void cv.dnn.Net.getMemoryConsumption [ARG int layerId=, ARG vector_MatShape netInputShapes=, ARG size_t weights=, ARG size_t blobs=]>
java: void getMemoryConsumption(int layerId, List<MatOfInt> netInputShapes, long[] weights, long[] blobs)
FUNC <void cv.dnn.Net.setHalideScheduler [ARG String scheduler=]>
java: void setHalideScheduler(String scheduler)
FUNC <void cv.dnn.Net.setInput [ARG Mat blob=, ARG String name=""]>
java: void setInput(Mat blob, String name)
java: void setInput(Mat blob)
FUNC <void cv.dnn.Net.setInputsNames [ARG vector_String inputBlobNames=]>
java: void setInputsNames(List<String> inputBlobNames)
FUNC <void cv.dnn.Net.setParam [ARG LayerId layer=, ARG int numParam=, ARG Mat blob=]>
java: void setParam(DictValue layer, int numParam, Mat blob)
FUNC <void cv.dnn.Net.setPreferableBackend [ARG int backendId=]>
java: void setPreferableBackend(int backendId)
FUNC <void cv.dnn.Net.setPreferableTarget [ARG int targetId=]>
java: void setPreferableTarget(int targetId)
CLASS cv.dnn::.DictValue : 
FUNC < cv.dnn.DictValue.DictValue [ARG String s=]>
java:  DictValue(String s)
FUNC < cv.dnn.DictValue.DictValue [ARG double p=]>
java:  DictValue(double p)
FUNC < cv.dnn.DictValue.DictValue [ARG int i=]>
java:  DictValue(int i)
FUNC <String cv.dnn.DictValue.getStringValue [ARG int idx=-1]>
java: String getStringValue(int idx)
java: String getStringValue()
FUNC <bool cv.dnn.DictValue.isInt []>
java: boolean isInt()
FUNC <bool cv.dnn.DictValue.isReal []>
java: boolean isReal()
FUNC <bool cv.dnn.DictValue.isString []>
java: boolean isString()
FUNC <double cv.dnn.DictValue.getRealValue [ARG int idx=-1]>
java: double getRealValue(int idx)
java: double getRealValue()
FUNC <int cv.dnn.DictValue.getIntValue [ARG int idx=-1]>
java: int getIntValue(int idx)
java: int getIntValue()
